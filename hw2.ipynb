{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "class InformationRetrievalSystem:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.documents = {}  # store the content of documents\n",
    "        self.boolean_index = defaultdict(set) \n",
    "        self.vector_index = {}\n",
    "        self.document_frequencies = Counter()\n",
    "        self.load_documents()\n",
    "        self.build_indexes()\n",
    "\n",
    "    def load_documents(self):\n",
    "        for filename in os.listdir(self.directory):\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().lower()\n",
    "                self.documents[filename] = content\n",
    "\n",
    "    def build_indexes(self):\n",
    "        for doc_id, text in self.documents.items():\n",
    "            words = set(text.split()) #unique words\n",
    "            for word in words: \n",
    "                self.boolean_index[word].add(doc_id) #step 1 : assemble sequence <token, docID>\n",
    "                #step 2 : boolean_index store the set of documents containing the word (ascending order)\n",
    "                #step 2 : merge multiple term entries per document\n",
    "                \n",
    "                self.document_frequencies[word] += 1 #step 3 add document frequency\n",
    "        \n",
    "        for doc_id, text in self.documents.items():\n",
    "            term_frequencies = Counter(text.split())\n",
    "            max_freq = max(term_frequencies.values()) \n",
    "            doc_vector = {}\n",
    "            for term, freq in term_frequencies.items():\n",
    "                tf = freq / max_freq  # Normalize TF\n",
    "                idf = math.log(len(self.documents) / (1 + self.document_frequencies[term])) #1+ to avoid division by 0\n",
    "                doc_vector[term] = tf * idf  # Calcul TF-IDF\n",
    "            self.vector_index[doc_id] = doc_vector\n",
    "\n",
    "\n",
    "    def boolean_search(self, query):\n",
    "        tokens = re.split(r'\\s+', query.upper())\n",
    "        result_set = None\n",
    "        current_op = \"AND\"\n",
    "\n",
    "        for i, token in enumerate(tokens): #merge postings\n",
    "            if token in {\"AND\", \"OR\", \"NOT\"}:\n",
    "                current_op = token\n",
    "            else:\n",
    "                matching_docs = self.boolean_index.get(token.lower(), set())\n",
    "                if result_set is None:\n",
    "                    result_set = matching_docs\n",
    "                else:\n",
    "                    if current_op == \"AND\":\n",
    "                        result_set &= matching_docs\n",
    "                    elif current_op == \"OR\":\n",
    "                        result_set |= matching_docs\n",
    "                    elif current_op == \"NOT\":\n",
    "                        result_set -= matching_docs\n",
    "\n",
    "        return result_set if result_set is not None else set()\n",
    "\n",
    "    def exact_match_search(self, query):\n",
    "        results = [doc_id for doc_id, text in self.documents.items() if query.lower() in text]\n",
    "        return results\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_terms = query.lower().split()\n",
    "        query_vector = {term: math.log(len(self.documents) / (1 + self.document_frequencies[term])) for term in query_terms} #tf_idf for each term\n",
    "        \n",
    "        scores = {}\n",
    "        for doc_id, doc_vector in self.vector_index.items():\n",
    "            score = sum(query_vector.get(term, 0) * doc_vector.get(term, 0) for term in query_terms) #cosine similarity\n",
    "            scores[doc_id] = score\n",
    "        \n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True) #sort by score in descending order\n",
    "\n",
    "    def search(self, query):\n",
    "        start_time = time.time()\n",
    "        boolean_results = self.boolean_search(query)\n",
    "        boolean_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        exact_results = self.exact_match_search(query)\n",
    "        exact_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        vector_results = self.vector_search(query)\n",
    "        vector_time = time.time() - start_time\n",
    "\n",
    "        print(\"--- Résults ---\")\n",
    "        print(f\"Booleen model ({len(boolean_results)} résults) : {boolean_results} en {boolean_time:.4f} sec\")\n",
    "        print(f\"Exact search ({len(exact_results)} résults) : {exact_results} en {exact_time:.4f} sec\")\n",
    "        print(f\"Vector space model ({len(vector_results)} résults) : {[doc for doc, _ in vector_results]} in {vector_time:.4f} sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Résultats ---\n",
      "Modèle booléen (1 résultats) : {'Carrot Cake Pancakes Carrot.txt'} en 0.0000 sec\n",
      "Recherche exacte (0 résultats) : [] en 0.0003 sec\n",
      "Modèle vectoriel (136 résultats) : ['Carrot Cake Pancakes Carrot.txt', 'Easy Bacon Pancake Sticks.txt', 'Carrot Cake Baked Oatmeal.txt', 'Easy Sausage Pancakes.txt', 'Carrot Bread This carrot.txt', 'Easy McGriddle Casserole.txt', 'High Protein Scrambled.txt', 'Bacon Pancakes These bacon.txt', 'Easter Egg Rolls These.txt', 'Pistachio Chip Ice Cream.txt', 'Crescent Sausage Egg Roll.txt', 'No Bake Espresso Martini.txt', 'German Chocolate Cake.txt', 'Pavlova Christmas Trees.txt', 'Watermelon Fruit Bowl.txt', 'Orange Meringue Pie A.txt', 'Cauliflower Popcorn Cauliflower.txt', 'Chef John s Tortilla de.txt', 'Lemon Turmeric Crinkles.txt', 'Strawberry Shortcake This.txt', 'Breakfast Nachos These.txt', 'Ramen Omelet with Spinach.txt', 'Flat Croissant with Nutella.txt', 'Christmas Bark This Christmas.txt', 'Brioche French Toast Casserole.txt', 'Cherry Cheesecake Dump.txt', 'Leftover Mashed Potato.txt', 'Croque Monsieur Casserole.txt', 'Pumpkin Oreo Cupcakes.txt', 'Cinnamon Roll Focaccia.txt', 'Ham and Gruyère Scones.txt', 'Italian Ricotta Cookie.txt', 'Crêpes Suzette Crêpes.txt', 'Nutella Swirl Cheesecake.txt', 'Funfetti Cheesecake Sandwich.txt', 'Strawberries and Cream.txt', 'Café Tropical Burrito.txt', 'Shortbread Crust Simple.txt', 'Pistachio Chocolate Baklava.txt', 'Whole Orange Blender Cake.txt', 'Lemon Poppy Seed Pancakes.txt', 'Blackout Cake Blackout.txt', 'Citrus and Pomegranate.txt', 'NYC Deli Bacon Egg and.txt', 'Smoked Salmon Breakfast.txt', 'Whipped Banana Ice Cream.txt', 'Million Dollar Bacon This.txt', 'Puppy Chow Crispy Treats.txt', 'Tiramisu Blondies These.txt', 'Chorizo Breakfast Casserole.txt', 'Hard Candy This hard candy.txt', 'Dubai Chocolate Berry.txt', 'Pistachio Cream Cinnamon.txt', 'Bananas Foster Crispy.txt', 'Spanish Style French Toast.txt', 'Granola Bars This granola.txt', 'Mexican Oatmeal This Mexican.txt', 'Baked Mini Doughnuts Yummy.txt', 'Full English Breakfast.txt', 'Asparagus and Eggs Asparagus.txt', 'Boursin Omelet This Boursin.txt', 'Fresh Rhubarb Pie This.txt', 'Roscoe s Chilaquiles Roscoe.txt', 'Christmas Cheesecake Bars.txt', 'Red Velvet Pots de Crème.txt', 'Sweet Potato Brownies.txt', 'Strawberry Spinach Salad.txt', 'Overnight Baked Oatmeal.txt', 'Lemon Crumb Bars These.txt', 'Sticky Toffee Pudding.txt', 'Baked Feta Eggs These.txt', 'Sticky Rice with Passion.txt', 'Halloumi Breakfast Sandwiches.txt', 'Ultimate Pumpkin Custard.txt', 'Fluffy Cake Donuts This.txt', 'Honey Wheat Bread II Everybody.txt', 'Brown Butter Caramel Chai.txt', 'Dubai Chocolate Strawberry.txt', 'Reese s Cup Overnight.txt', 'Cookies and Cream Rice.txt', 'Cherry Torte with Cherry.txt', 'Shavuot Cheesecake This.txt', 'Orange Cinnamon Rolls.txt', 'French Silk Pie Bars These.txt', 'Pineapple Stuffing This.txt', 'Balsamic Grape and Walnut.txt', 'Chocolate Chip Cookie.txt', 'Rainbow Clown Cake I went.txt', 'Easy Pecan Sticky Buns.txt', 'Easy Strawberry Brownies.txt', 'Espresso Brownies Adding.txt', 'Tartufo Tartufo is a classic.txt', 'Fresh Cranberry Sauce.txt', 'Churros Churros Mexican.txt', 'Salted Caramel Irish Cream.txt', 'Mochi Donuts These homemade.txt', 'Oatmeal Almond Butter.txt', 'Coffee Creamer French.txt', 'Boursin Scrambled Eggs.txt', 'Rhubarb Buttermilk Clafoutis.txt', 'Granola Cups These granola.txt', 'Lemon Cheesecake Creme.txt', 'Apple Cider Cut Out Cookies.txt', 'Chocolate Protein Balls.txt', 'Caramel Apple Overnight.txt', 'Cinnamon Nutmeg Coffee.txt', 'Fudgy Black Forest Bars.txt', 'Grilled Peanut Butter.txt', 'Cottage Cheese Blueberry.txt', 'Mandarin Orange Watergate.txt', 'Sleeping Gingerbread Treats.txt', 'Sausage and Cheese Egg.txt', 'Everything Bagel Grilled.txt', 'Pistachio Torte This nostalgic.txt', 'Baked Bananas Many GPers.txt', 'Pecan Pie Energy Bites.txt', 'Creepy Halloween Skull.txt', 'Microwave Chocolate Pudding.txt', 'The Greatest Apple Crisp.txt', 'Individual Bourbon Pecan.txt', 'Ezekiel Bread This Ezekiel.txt', 'Jam Jewel Cookies Light.txt', 'Banana Chocolate Chip.txt', 'Oreo Cookie Pie Crust.txt', 'Chilaquiles Breakfast.txt', 'Berry Fruit Salad Mixture.txt', 'Blueberry Baked Oats These.txt', 'Cheesecake Crepe Roll.txt', 'Crispy Fried Poached Eggs.txt', 'Struffoli Struffoli are.txt', 'Broiled Mochi with Nori.txt', 'Apple Snicker Salad Had.txt', 'Tahini Butter Cookies.txt', 'Coconut Milk Ice Cream.txt', 'TikTok Croiffles The easiest.txt', 'Air Fryer S Mores This.txt'] en 0.0004 sec\n"
     ]
    }
   ],
   "source": [
    "ir_system = InformationRetrievalSystem(\"recipes\")\n",
    "while True:\n",
    "    query = input(\"Enter your request (or 'exit') : \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    ir_system.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubiquity-student-wDpXH6Hs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
